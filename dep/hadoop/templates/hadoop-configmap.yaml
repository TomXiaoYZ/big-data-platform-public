apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "hadoop.fullname" . }}
  labels:
    app.kubernetes.io/name: {{ include "hadoop.name" . }}
    helm.sh/chart: {{ include "hadoop.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
data:
  bootstrap.sh: |
    #!/bin/bash -x

    echo Starting

    : ${HADOOP_CONF_DIR:=/etc/hadoop/conf}
    : ${HIVE_CONF_DIR:=/etc/hive/conf}

    echo Using ${HADOOP_CONF_DIR} as HADOOP_CONF_DIR
    echo Using ${HIVE_CONF_DIR} as HIVE_CONF_DIR

    . $HADOOP_CONF_DIR/hadoop-env.sh

    # ------------------------------------------------------
    # Directory to find config artifacts
    # ------------------------------------------------------

    CONFIG_DIR="/tmp/hadoop-config"

    # ------------------------------------------------------
    # Copy config files from volume mount
    # ------------------------------------------------------

    for f in slaves core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml; do
      if [[ -e ${CONFIG_DIR}/$f ]]; then
        cp ${CONFIG_DIR}/$f $HADOOP_CONF_DIR/$f
      else
        echo "ERROR: Could not find $f in $CONFIG_DIR"
        exit 1
      fi
    done

    for f in hive-site.xml; do
      if [[ -e ${CONFIG_DIR}/$f ]]; then
        cp ${CONFIG_DIR}/$f $HIVE_CONF_DIR/$f
      else
        echo "ERROR: Could not find $f in $CONFIG_DIR"
        exit 1
      fi
    done

    # ------------------------------------------------------
    # installing libraries if any
    # (resource urls added comma separated to the ACP system variable)
    # ------------------------------------------------------
    # cd $HADOOP_HOME/share/hadoop/common ; for cp in ${ACP//,/ }; do  echo == $cp; curl -LO $cp ; done; cd -

    # ------------------------------------------------------
    # Start NAMENODE
    # ------------------------------------------------------
    if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then
      # cd ${RANGER_HOME}/ranger-hdfs-plugin
      # ./enable-hdfs-plugin.sh

      # sed command changing REPLACEME in $HADOOP_CONF_DIR/hdfs-site.xml to actual port numbers
      sed -i "s/EXTERNAL_HTTP_PORT_REPLACEME/9864/" $HADOOP_CONF_DIR/hdfs-site.xml
      sed -i "s/EXTERNAL_DATA_PORT_REPLACEME/9866/" $HADOOP_CONF_DIR/hdfs-site.xml

      mkdir -p /root/hdfs/namenode
      if [ ! -f /root/hdfs/namenode/formated ]; then
        # Only format if necessary
        hdfs namenode -format -force -nonInteractive && echo 1 > /root/hdfs/namenode/formated
      fi
      hdfs --loglevel {{ .Values.logLevel }} --daemon start namenode
    fi

    # ------------------------------------------------------
    # Start DATA NODE
    # ------------------------------------------------------
    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
      # Split hostname at "-" into an array
      # Example hostname: hadoop-hadoop-hdfs-dn-0
      HOSTNAME_ARR=(${HOSTNAME//-/ })
      # Add instance number to start of external port ranges
      EXTERNAL_HTTP_PORT=$(({{ .Values.hdfs.dataNode.externalHTTPPortRangeStart }} + ${HOSTNAME_ARR[4]}))
      EXTERNAL_DATA_PORT=$(({{ .Values.hdfs.dataNode.externalDataPortRangeStart }} + ${HOSTNAME_ARR[4]}))

      # sed command changing REPLACEME in $HADOOP_CONF_DIR/hdfs-site.xml to actual port numbers
      sed -i "s/EXTERNAL_HTTP_PORT_REPLACEME/${EXTERNAL_HTTP_PORT}/" $HADOOP_CONF_DIR/hdfs-site.xml
      sed -i "s/EXTERNAL_DATA_PORT_REPLACEME/${EXTERNAL_DATA_PORT}/" $HADOOP_CONF_DIR/hdfs-site.xml

      mkdir -p /root/hdfs/datanode

      #  Wait (with timeout) for namenode
      TMP_URL="http://{{ include "hadoop.fullname" . }}-hdfs-nn:9870"
      if timeout 5m bash -c "until curl -sf $TMP_URL; do echo Waiting for $TMP_URL; sleep 5; done"; then
        hdfs --loglevel {{ .Values.logLevel }} --daemon start datanode
      else 
        echo "$0: Timeout waiting for $TMP_URL, exiting."
        exit 1
      fi

    fi

    # ------------------------------------------------------
    # Start RESOURCE MANAGER and PROXY SERVER as daemons
    # ------------------------------------------------------
    if [[ "${HOSTNAME}" =~ "yarn-rm" ]]; then
      yarn --loglevel {{ .Values.logLevel }} --daemon start resourcemanager
      yarn --loglevel {{ .Values.logLevel }} --daemon start proxyserver
    fi

    # ------------------------------------------------------
    # Start NODE MANAGER
    # ------------------------------------------------------
    if [[ "${HOSTNAME}" =~ "yarn-nm" ]]; then
      # cd ${RANGER_HOME}/ranger-yarn-plugin
      # ./enable-yarn-plugin.sh

      sed -i '/<\/configuration>/d' $HADOOP_CONF_DIR/yarn-site.xml
      cat >> $HADOOP_CONF_DIR/yarn-site.xml <<- EOM
      <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>${MY_MEM_LIMIT:-2048}</value>
      </property>

      <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>${MY_CPU_LIMIT:-2}</value>
      </property>
    EOM

      echo '</configuration>' >> $HADOOP_CONF_DIR/yarn-site.xml

      # Wait with timeout for resourcemanager
      TMP_URL="http://{{ include "hadoop.fullname" . }}-yarn-rm:8088/ws/v1/cluster/info"
      if timeout 5m bash -c "until curl -sf $TMP_URL; do echo Waiting for $TMP_URL; sleep 5; done"; then
        yarn nodemanager --loglevel {{ .Values.logLevel }}
      else 
        echo "$0: Timeout waiting for $TMP_URL, exiting."
        exit 1
      fi

    fi

    # ------------------------------------------------------
    # Start HIVE METASTORE & HIVE SERVER2
    # ------------------------------------------------------
    if [[ "${HOSTNAME}" =~ "hive" ]]; then
      cd ${RANGER_HOME}/ranger-hive-plugin
      ./enable-hive-plugin.sh

      cd $HIVE_HOME
      export HADOOP_USER_CLASSPATH_FIRST=true
      hdfs dfs -mkdir /tmp
      hdfs dfs -chmod g+w /tmp
      hdfs dfs -mkdir -p /user/hive/warehouse
      hdfs dfs -chmod g+w /user/hive/warehouse
{{/*      nohup ${HIVE_HOME}/bin/hive --service metastore > metastore.log 2>&1 &*/}}
{{/*      hive -l -c 'HIVE_CONF_DIR=/etc/hive/conf /usr/hdp/current/hive-server2/bin/hiveserver2 -hiveconf hive.metastore.uris=" " -hiveconf hive.log.dir=/var/log/hive -hiveconf hive.log.file=hiveserver2.log 1>/var/log/hive/hiveserver2.log 2>/var/log/hive/hiveserver2.log &'*/}}
      nohup hive --service hiveserver2 > hive-server2.log 2>&1 &
    fi
    # ------------------------------------------------------
    # Tail logfiles for daemonized workloads (parameter -d)
    # ------------------------------------------------------
    if [[ $1 == "-d" ]]; then
      if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then
        log_dir=/var/log/hadoop-hdfs
      elif [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
        log_dir=/var/log/hadoop-hdfs
      elif [[ "${HOSTNAME}" =~ "hive" ]]; then
        log_dir=$HIVE_HOME
      else
        log_dir=/usr/lib/hadoop/logs
      fi
      until find $log_dir/ -mmin -1 | egrep -q '.*'; echo "`date`: Waiting for logs..." ; do sleep 2 ; done
      tail -F $log_dir/* &
      while true; do sleep 1000; done
    fi

    # ------------------------------------------------------
    # Start bash if requested (parameter -bash)
    # ------------------------------------------------------
    if [[ $1 == "-bash" ]]; then
      /bin/bash
    fi

  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://{{ include "hadoop.fullname" . }}-hdfs-nn:9000/</value>
            <description>NameNode URI</description>
        </property>
        <property>
            <name>hadoop.security.authentication</name>
            <value>simple</value>
        </property>
        <property>
            <name>hadoop.security.group.mapping</name>
            <value>org.apache.hadoop.security.LdapGroupsMapping</value>
        </property>
        <property>
            <name>hadoop.security.group.mapping.ldap.bind.user</name>
            <value>{{ .Values.ldap.bind_dn }}</value>
        </property>
        <property>
            <name>hadoop.security.group.mapping.ldap.bind.password</name>
            <value>{{ .Values.ldap.bind_password }}</value>
        </property>
        <property>
            <name>hadoop.security.group.mapping.ldap.url</name>
            <value>{{ .Values.ldap.url }}</value>
        </property>
    </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>

{{- if .Values.hdfs.webhdfs.enabled -}}
        <property>
            <name>dfs.webhdfs.enabled</name>
            <value>true</value>
        </property>
{{- end -}}

        <property>
            <name>dfs.datanode.use.datanode.hostname</name>
            <value>false</value>
        </property>

        <property>
            <name>dfs.client.use.datanode.hostname</name>
            <value>false</value>
        </property>

        <property>
            <name>dfs.datanode.hostname</name>
            <value>{{ .Values.hdfs.dataNode.externalHostname }}</value>
        </property>

        <property>
            <name>dfs.datanode.http.address</name>
            <value>0.0.0.0:EXTERNAL_HTTP_PORT_REPLACEME</value>
        </property>

        <property>
            <name>dfs.datanode.address</name>
            <value>0.0.0.0:EXTERNAL_DATA_PORT_REPLACEME</value>
        </property>

        <property>
            <name>dfs.replication</name>
            <value>3</value>
        </property>

        <property>
            <name>dfs.datanode.data.dir</name>
            <value>file:///root/hdfs/datanode</value>
            <description>DataNode directory</description>
        </property>

        <property>
            <name>dfs.namenode.name.dir</name>
            <value>file:///root/hdfs/namenode</value>
            <description>NameNode directory for namespace and transaction logs storage.</description>
        </property>

        <property>
            <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
            <value>false</value>
        </property>

        <!-- Bind to all interfaces -->
        <property>
            <name>dfs.namenode.rpc-bind-host</name>
            <value>0.0.0.0</value>
        </property>
        <property>
            <name>dfs.namenode.servicerpc-bind-host</name>
            <value>0.0.0.0</value>
        </property>
        <!-- /Bind to all interfaces -->

    </configuration>

  mapred-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
        <property>
            <name>mapreduce.jobhistory.address</name>
            <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm.{{ .Release.Namespace }}.svc.cluster.local:10020</value>
        </property>
        <property>
            <name>mapreduce.jobhistory.webapp.address</name>
            <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm.{{ .Release.Namespace }}.svc.cluster.local:19888</value>
        </property>
    </configuration>

  slaves: |
    localhost

  yarn-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
        <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>{{ include "hadoop.fullname" . }}-yarn-rm</value>
        </property>

        <!-- Bind to all interfaces -->
        <property>
            <name>yarn.resourcemanager.bind-host</name>
            <value>0.0.0.0</value>
        </property>
        <property>
            <name>yarn.nodemanager.bind-host</name>
            <value>0.0.0.0</value>
        </property>
        <property>
            <name>yarn.timeline-service.bind-host</name>
            <value>0.0.0.0</value>
        </property>
        <property>
            <name>yarn.web-proxy.address</name>
            <value>0.0.0.0</value>
        </property>
        <!-- /Bind to all interfaces -->

        <property>
            <name>yarn.nodemanager.vmem-check-enabled</name>
            <value>false</value>
        </property>

        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>

        <property>
            <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
            <value>org.apache.hadoop.mapred.ShuffleHandler</value>
        </property>

        <property>
            <description>List of directories to store localized files in.</description>
            <name>yarn.nodemanager.local-dirs</name>
            <value>/var/lib/hadoop-yarn/cache/${user.name}/nm-local-dir</value>
        </property>

        <property>
            <description>Where to store container logs.</description>
            <name>yarn.nodemanager.log-dirs</name>
            <value>/var/log/hadoop-yarn/containers</value>
        </property>

        <property>
            <description>Where to aggregate logs to.</description>
            <name>yarn.nodemanager.remote-app-log-dir</name>
            <value>/var/log/hadoop-yarn/apps</value>
        </property>

        <property>
            <name>yarn.application.classpath</name>
            <value>
                $HADOOP_CONF_DIR,
                $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
                $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,
                $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,
                $HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
                $HIVE_HOME/*,$HIVE_HOME/lib/*,
                /usr/lib/hadoop-lzo/lib/*,
                /usr/share/aws/emr/emrfs/conf,
                /usr/share/aws/emr/emrfs/lib/*,
                /usr/share/aws/emr/emrfs/auxlib/*,
                /usr/share/aws/emr/lib/*,
                /usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar,
                /usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar,
                /usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar,
                /usr/share/aws/emr/cloudwatch-sink/lib/*,
                /usr/share/aws/aws-java-sdk/*
            </value>
        </property>
    </configuration>

  hive-site.xml: |
    <configuration>
        <property>
            <name>hive.zookeeper.quorum</name>
            <value>zookeeper.hadoop.svc.cluster.locol:2181</value>
        </property>
        <property>
            <name>hive.llap.zk.sm.connectionString</name>
            <value>zookeeper.hadoop.svc.cluster.locol:2181</value>
        </property>

        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://{{ include "hadoop.fullname" . }}-hdfs-nn:9000/</value>
            <description>NameNode URI</description>
        </property>
{{/*        <property>*/}}
{{/*            <name>hive.metastore.uris</name>*/}}
{{/*            <value>thrift://0.0.0.0:9083</value>*/}}
{{/*        </property>*/}}
        <property>
            <name>javax.jdo.option.ConnectionURL</name>
            <value>jdbc:mysql://{{ .Values.hive.mysql.endpoint }}:{{ .Values.hive.mysql.port }}/{{ .Values.hive.mysql.database }}</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionDriverName</name>
            <value>com.mysql.jdbc.Driver</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionUserName</name>
            <value>{{ .Values.hive.mysql.username }}</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionPassword</name>
            <value>{{ .Values.hive.mysql.password }}</value>
        </property>
        <property>
            <name>hive.server2.allow.user.substitution</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.server2.enable.doAs</name>
            <value>false</value>
        </property>
        <property>
            <name>hive.server2.thrift.bind.host</name>
            <value>0.0.0.0</value>
        </property>
        <property>
            <name>hive.server2.thrift.port</name>
            <value>10000</value>
        </property>
        <property>
            <name>hive.server2.transport.mode</name>
            <value>http</value>
        </property>
        <property>
            <name>hive.server2.thrift.http.port</name>
            <value>10001</value>
        </property>
        <property>
            <name>hive.optimize.ppd.input.formats</name>
            <value>com.amazonaws.emr.s3select.hive.S3SelectableTextInputFormat</value>
        </property>
        <property>
            <name>s3select.filter</name>
            <value>false</value>
        </property>
        <property>
            <name>hive.server2.authentication.ldap.url</name>
            <value>{{ .Values.ldap.url }}</value>
        </property>
        <property>
            <name>hive.server2.authentication.ldap.baseDN</name>
            <value>{{ .Values.ldap.base_dn }}</value>
        </property>
        <property>
            <name>hive.server2.authentication</name>
            <value>LDAP</value>
        </property>
        <property>
            <name>hive.metastore.warehouse.dir</name>
            <value>{{ .Values.hive.warehouse.location }}</value>
        </property>
        <property>
            <name>hive.server2.authentication.ldap.userDNPattern</name>
            <value>cn=%s,{{ .Values.ldap.base_dn }}</value>
        </property>
        <property>
            <name>hive.execution.engine</name>
            <value>tez</value>
        </property>
        <property>
            <name>fs.s3a.buffer.dir</name>
            <value>/tmp</value>
        </property>
        <property>
            <name>fs.s3.impl</name>
            <value>com.amazon.ws.emr.hadoop.fs.EmrFileSystem</value>
        </property>
        <property>
            <name>fs.s3.impl</name>
            <value>com.amazon.ws.emr.hadoop.fs.EmrFileSystem</value>
        </property>
        <property>
            <name>fs.s3n.impl</name>
            <value>com.amazon.ws.emr.hadoop.fs.EmrFileSystem</value>
        </property>
        <property>
            <name>fs.AbstractFileSystem.s3.impl</name>
            <value>org.apache.hadoop.fs.s3.EMRFSDelegate</value>
        </property>
        <property>
            <name>fs.s3bfs.impl</name>
            <value>org.apache.hadoop.fs.s3.EMRFSDelegate</value>
        </property>
    </configuration>